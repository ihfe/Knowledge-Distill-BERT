# -*- coding: utf-8 -*-
"""å¢é‡6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Kvs_IW57AKACNu-LzGspTNLvzARRhPCm

æœ¬NoteBookä»‹ç»äº†BERTç›¸å…³æ¨¡å‹å’ŒçŸ¥è¯†è’¸é¦è¿‡ç¨‹

æ•°æ®é›†ï¼šCoLL2003,åˆ†ç±»æ•°æ®é›†

å…ˆæŸ¥çœ‹ä¸åŒå‚æ•°é‡çš„æ¨¡å‹bert-base-uncasedã€DistilBERTã€TinyBERTæ¨¡å‹åœ¨è¯¥æ•°æ®é›†ä¸Šçš„è¡¨ç°æƒ…å†µï¼›

æ­¤å¤–ï¼Œåˆ†æäº†ä¸¤ç§å­¦ç”Ÿæ¨¡å‹çš„æƒ…å†µï¼š
- é¢„è®­ç»ƒæ¨¡å‹TinyBERTä½œä¸ºå­¦ç”Ÿæ¨¡å‹æ—¶ï¼Œæ¨¡å‹çš„è¡¨ç°æƒ…å†µ
- è‡ªå®šä¹‰BERTæ¨¡å‹ç»“æ„ä½œä¸ºå­¦ç”Ÿæ¨¡å‹ï¼Œé€‰æ‹©DistilBERTæ¨¡å‹çš„éƒ¨åˆ†å±‚å‚æ•°ï¼Œç„¶åè§‚å¯Ÿè¡¨ç°æƒ…å†µ
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader,Dataset
from transformers import BertTokenizer,BertModel

current_tag2id = {
    'O': 0,
    'B-PER': 1,
    'I-PER': 2,
    'B-ORG': 3,
    'I-ORG': 4,
    'B-LOC': 5,
    'I-LOC': 6,
    'B-MISC': 7,
    'I-MISC': 8
}

def parse_conll2003(file_path):
    sentences, labels = [], []
    sentence, label = [], []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip() == '':
                if sentence:
                    sentences.append(sentence)
                    labels.append(label)
                    sentence, label = [], []
            else:
                token, _, _, tag = line.strip().split()
                sentence.append(token)
                label.append(tag)
    return sentences, labels

file_path = "../content/test.txt"
sentences,labels = parse_conll2003(file_path)
sentences_test = sentences[:1500]
labels_test = labels[:1500]

file_path = "../content/train.txt"
sentences,labels = parse_conll2003(file_path)#len(sentences) = 14987
sentences[1],labels[1]

class NERDataset(Dataset):
    def __init__(self, sentences, labels, tokenizer, max_len=128, current_tag2id=current_tag2id):
        self.sentences = sentences
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len
        self.current_tag2id = current_tag2id or {'O': 0}

    def __len__(self):
        return len(self.sentences)

    def __getitem__(self, idx):
        sentence = self.sentences[idx]
        label = self.labels[idx]
        tokens = []
        label_ids = []
        for word, tag in zip(sentence, label):
            word_tokens = self.tokenizer.tokenize(word)
            if word_tokens:
                tokens.extend(word_tokens)
                label_ids.extend([self.current_tag2id.get(tag, 0)] + [-100] * (len(word_tokens) - 1))
        tokens = ['[CLS]'] + tokens + ['[SEP]']
        label_ids = [-100] + label_ids + [-100]
        if len(tokens) > self.max_len:
            tokens = tokens[:self.max_len]
            label_ids = label_ids[:self.max_len]
        else:
            padding_len = self.max_len - len(tokens)
            tokens += ['[PAD]'] * padding_len
            label_ids += [-100] * padding_len
        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)
        attention_mask = [1 if token != '[PAD]' else 0 for token in tokens]
        return {
            'input_ids': torch.tensor(input_ids),
            'attention_mask': torch.tensor(attention_mask),
            'labels': torch.tensor(label_ids)
        }

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
Dataset = NERDataset(sentences,labels,tokenizer)

from torch.utils.data import DataLoader

data_loader = DataLoader(Dataset,batch_size=32,shuffle=True)
for loader in data_loader:
    a = loader["input_ids"]
    print(len(a))
    break
len(data_loader)

Dataset_test = NERDataset(sentences_test,labels_test,tokenizer)
data_loader_test = DataLoader(Dataset_test,batch_size=32,shuffle=True)

class BertNER(nn.Module):
    def __init__(self, model,num_labels):
        super(BertNER, self).__init__()
        self.bert = model
        self.dropout = nn.Dropout(0.1)
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)
        self.num_labels = num_labels

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids, attention_mask=attention_mask)
        sequence_output = outputs[0]
        sequence_output = self.dropout(sequence_output)
        logits = self.classifier(sequence_output)#å¦‚æœæ˜¯10ç±»ï¼Œè¿™ä¸ªlogitsæ˜¯å‘é‡ï¼Ÿlogitsæ˜¯æœªå½’ä¸€åŒ–çš„åŸå§‹å€¼ï¼Œå€¼å¯æ­£å¯è´Ÿ
        return logits

    def extend_output_layer(self, new_labels):
        # åŠ¨æ€æ‰©å±•è¾“å‡ºå±‚ä»¥é€‚åº”æ–°å®ä½“ç±»å‹
        new_num_labels = self.num_labels + len(new_labels)
        old_classifier = self.classifier
        self.classifier = nn.Linear(self.bert.config.hidden_size, new_num_labels)
        with torch.no_grad():
            self.classifier.weight[:old_classifier.weight.size(0)] = old_classifier.weight
            self.classifier.bias[:old_classifier.bias.size(0)] = old_classifier.bias
        self.num_labels = new_num_labels

from torch.nn.functional import kl_div, log_softmax, softmax

def train_with_distillation(model, teacher_model, dataloader, optimizer, device, alpha=0.3, T_m=2.0):
    model.train()
    if teacher_model:
        teacher_model.eval()
    total_loss = 0
    # student_loss = []
    # teacher_loss = []
    criterion = nn.CrossEntropyLoss(ignore_index=-100)
    for step, batch in enumerate(dataloader, 1):#len(dataloader) = 469
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        logits = model(input_ids, attention_mask)
        ce_loss = criterion(logits.view(-1, model.num_labels), labels.view(-1))
        if teacher_model:
            with torch.no_grad():
                teacher_logits = teacher_model(input_ids, attention_mask)
            kl_loss = kl_div(log_softmax(logits / T_m, dim=-1),
                            softmax(teacher_logits / T_m, dim=-1),
                            reduction='batchmean') * (T_m ** 2)
            loss = alpha * kl_loss + (1-alpha) * ce_loss
            # student_loss.append(loss.item())
        else:
            loss = ce_loss
            # teacher_loss.append(loss.item())
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        total_loss += loss.item()#lossæ˜¯å¼ é‡ï¼Œå¼ é‡è½¬æ ‡é‡ï¼Œç„¶åç›¸åŠ 
        if step == 1 or step % 50 == 0 or step == len(dataloader):
          print(f"[{step}/{len(dataloader)}] Loss: {loss.item():.5f}")
    return total_loss / len(dataloader)

torch.cuda.is_available()

"""ğŸ“Œè®­ç»ƒä¸€ä¸ªå¤§æ¨¡å‹ï¼ˆbert-base-uncasedæ¨¡å‹ï¼‰ä½œä¸ºæ•™å¸ˆæ¨¡å‹ï¼Œæ•ˆæœæ˜¯æ€ä¹ˆæ ·çš„å‘¢ï¼Ÿ"""

import torch.optim as optim
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
bert_model = BertModel.from_pretrained('bert-base-uncased')
teacher = BertNER(bert_model,num_labels=9).to(device)#åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
optimizer = optim.Adam(teacher.parameters(), lr=2e-5)
print(next(teacher.parameters()).device)

avg_loss_teacher = train_with_distillation(
    model=teacher,
    teacher_model=None,        # ä¼  None
    dataloader=data_loader,
    optimizer=optimizer,
    device=device,
    alpha=0.3,
    T_m=2.0
)
avg_loss_teacher

model_size = sum(param.numel() * param.element_size() for param in teacher.parameters())
print(f"Teacher Modelã€bert-base-uncasedã€‘ size: {model_size/(1024**2):.2f}MB")

total_loss_teacher = 0
criterion = nn.CrossEntropyLoss(ignore_index=-100)
for step, batch in enumerate(data_loader_test, 1):#len(dataloader) = 469
  input_ids = batch['input_ids'].to(device)
  attention_mask = batch['attention_mask'].to(device)
  labels = batch['labels'].to(device)
  logits = teacher(input_ids, attention_mask)
  ce_loss = criterion(logits.view(-1, teacher.num_labels), labels.view(-1))
  total_loss_teacher += ce_loss.item()
total_loss_teacher

"""å¯ä»¥çœ‹åˆ°ã€bert-base-uncasedã€‘ï¼š
- è®­ç»ƒæ—¶é•¿æ€»å…±èŠ±äº†5åˆ†é’Ÿï¼Œ
- æ•™å¸ˆæ¨¡å‹çš„å‚æ•°é‡å¤§å°ä¸º417.67MBï¼Œ
- æœ€ç»ˆçš„`avg_loss_teacher`ä¸º0.186å·¦å³
- æµ‹è¯•é›†æŸå¤±åœ¨4.037å·¦å³

ğŸ“Œå¦‚æœæˆ‘ç›´æ¥è®­ç»ƒä¸€ä¸ªå°æ¨¡å‹ï¼ˆdistilbertæ¨¡å‹ï¼‰ï¼Œæ•ˆæœæ˜¯æ€ä¹ˆæ ·çš„å‘¢ï¼Ÿ
"""

import torch.optim as optim
from transformers import DistilBertModel

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

DistilBERT_Student_model = DistilBertModel.from_pretrained('distilbert-base-uncased')
distilbert = BertNER(DistilBERT_Student_model,num_labels=9).to(device)#åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
optimizer = optim.Adam(distilbert.parameters(), lr=2e-5)
avg_loss_distilbert = train_with_distillation(
    model=distilbert,
    teacher_model=None,        # ä¼  None
    dataloader=data_loader,
    optimizer=optimizer,
    device=device,
    alpha=0.3,
    T_m=2.0
)
avg_loss_distilbert

# model_size = sum(param.numel() * param.element_size() for param in distilbert.parameters())
# print(f"Distilbert Model size: {model_size/(1024**2):.2f}MB")

#æŸ¥çœ‹ä¸€ä¸‹æ¨¡å‹å±‚æ•°ï¼Œå› ä¸ºå¾…ä¼šå„¿ï¼ˆæœ€åéƒ¨åˆ†ï¼‰æˆ‘ä»¬è¦æå–ç¬¬1å±‚ï¼Œç¬¬3å±‚ï¼Œç¬¬6å±‚,å³[0,2,5]
print(f"Encoder å±‚æ•°ï¼ˆfrom configï¼‰: {DistilBERT_Student_model.config.num_hidden_layers}")

total_loss_teacher = 0
criterion = nn.CrossEntropyLoss(ignore_index=-100)
for step, batch in enumerate(data_loader_test, 1):#len(dataloader) = 469
  input_ids = batch['input_ids'].to(device)
  attention_mask = batch['attention_mask'].to(device)
  labels = batch['labels'].to(device)
  logits = distilbert(input_ids, attention_mask)
  ce_loss = criterion(logits.view(-1, distilbert.num_labels), labels.view(-1))
  total_loss_teacher += ce_loss.item()
total_loss_teacher

"""å¯ä»¥çœ‹åˆ°ã€Distilbertã€‘ï¼š
- è®­ç»ƒæ—¶é•¿æ€»å…±èŠ±äº†2åˆ†é’Ÿï¼Œ
- å­¦ç”Ÿæ¨¡å‹çš„å‚æ•°é‡å¤§å°ä¸º253.18MBï¼Œ
- æœ€ç»ˆçš„`avg_loss_distilbert`ä¸º0.21å·¦å³
- æµ‹è¯•æŸå¤±åœ¨4.371å·¦å³

ğŸ“Œå¦‚æœæˆ‘ç›´æ¥è®­ç»ƒä¸€ä¸ªå°æ¨¡å‹ï¼ˆTinyBertæ¨¡å‹ï¼‰ï¼Œæ•ˆæœæ˜¯æ€ä¹ˆæ ·çš„å‘¢ï¼Ÿ
"""

from transformers import AutoModel

model = AutoModel.from_pretrained("huawei-noah/TinyBERT_General_4L_312D")
Tinybert = BertNER(model,num_labels=9).to(device)#åŠ è½½é¢„è®­ç»ƒæ¨¡å‹

optimizer = optim.Adam(Tinybert.parameters(), lr=2e-5)
avg_loss_Tinybert = train_with_distillation(
    model=Tinybert,
    teacher_model=None,        # ä¼ å…¥ None
    dataloader=data_loader,
    optimizer=optimizer,
    device=device,
    alpha=0.3,
    T_m=2.0
)
avg_loss_Tinybert

model_size = sum(param.numel() * param.element_size() for param in Tinybert.parameters())
print(f"Tinylbert Model size: {model_size/(1024**2):.2f}MB")

#æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ•ˆæœ
total_loss = 0
criterion = nn.CrossEntropyLoss(ignore_index=-100)
for step, batch in enumerate(data_loader_test, 1):#len(dataloader) = 469
  input_ids = batch['input_ids'].to(device)
  attention_mask = batch['attention_mask'].to(device)
  labels = batch['labels'].to(device)
  logits = Tinybert(input_ids, attention_mask)
  ce_loss = criterion(logits.view(-1, Tinybert.num_labels), labels.view(-1))
  total_loss += ce_loss.item()
total_loss

"""å¯ä»¥çœ‹åˆ°ã€TinyBertã€‘ï¼š
- è®­ç»ƒæ—¶é•¿æ€»å…±èŠ±äº†32ç§’ï¼Œ
- å­¦ç”Ÿæ¨¡å‹çš„å‚æ•°é‡å¤§å°ä¸º54.75MBï¼Œ
- æœ€ç»ˆçš„`avg_loss_TinyBert`ä¸º0.358å·¦å³
- æœ€ç»ˆæµ‹è¯•é›†æŸå¤±åœ¨9.417

ğŸ“Œå¥½ï¼Œæ¥ä¸‹æ¥å’±ä»¬å¼€å§‹è’¸é¦ï¼Œæ¥æŸ¥çœ‹ä¸€ä¸‹ä½¿ç”¨`TinyBert`ä½œä¸ºBaseLineå¾—åˆ°çš„å­¦ç”Ÿæ¨¡å‹æ˜¯ä¸æ˜¯è¦æ¯”ç›´æ¥è®­ç»ƒ`TinyBert`çš„æ•ˆæœè¦å¥½
"""

model = AutoModel.from_pretrained("huawei-noah/TinyBERT_General_4L_312D")
Tinybert_Student = BertNER(model,num_labels=9).to(device)#åŠ è½½é¢„è®­ç»ƒæ¨¡å‹

optimizer = optim.Adam(Tinybert_Student.parameters(), lr=2e-5)

avg_loss_student = train_with_distillation(
    model=Tinybert_Student,
    teacher_model=teacher,
    dataloader=data_loader,
    optimizer=optimizer,
    device=device,
    alpha=0.3,
    T_m=2.0
)

#æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ•ˆæœ
total_loss = 0
criterion = nn.CrossEntropyLoss(ignore_index=-100)
for step, batch in enumerate(data_loader_test, 1):#len(dataloader) = 469
  input_ids = batch['input_ids'].to(device)
  attention_mask = batch['attention_mask'].to(device)
  labels = batch['labels'].to(device)
  logits = Tinybert_Student(input_ids, attention_mask)
  ce_loss = criterion(logits.view(-1, Tinybert_Student.num_labels), labels.view(-1))
  total_loss += ce_loss.item()
total_loss

"""å¯ä»¥çœ‹åˆ°ã€Tinybert_Studentã€‘ï¼š
- è’¸é¦æ—¶é•¿èŠ±äº†2åˆ†é’Ÿï¼Œ
- å­¦ç”Ÿæ¨¡å‹çš„å‚æ•°é‡å¤§å°ä¾ç„¶ä¸º54.75MBï¼ˆå’ŒTinyBertä¸€æ ·ï¼‰
- æœ€ç»ˆæµ‹è¯•é›†æŸå¤±åœ¨9.2

ğŸ“Œå¥½ï¼Œæ¥ä¸‹æ¥å’±ä»¬æ¢ä¸€ä¸ªå­¦ç”Ÿæ¨¡å‹ï¼Œå°†Teacheræ¨¡å‹çš„éƒ¨åˆ†å±‚ã€ä¹Ÿå°±æ˜¯0ã€2ã€5å±‚ã€‘çš„æƒé‡é€šè¿‡æ·±æ‹·è´ï¼Œèµ‹å€¼ç»™è‡ªå®šä¹‰çš„å­¦ç”Ÿæ¨¡å‹
"""

import copy
from transformers import DistilBertModel, DistilBertConfig

# === 2. åˆ›å»ºå­¦ç”Ÿæ¨¡å‹é…ç½®ï¼ˆ6å±‚ï¼‰ ===
student_config = copy.deepcopy(DistilBERT_Student_model.config)
student_config.num_hidden_layers = 3
student_model = DistilBertModel(student_config)

# === 4. é€‰å–æ•™å¸ˆæ¨¡å‹ä¸­çš„å±‚ï¼ˆæ¯éš”ä¸€å±‚ï¼‰æ‹·è´åˆ°å­¦ç”Ÿæ¨¡å‹ ===
selected_teacher_layers = [0, 2, 5]

for student_layer_idx, teacher_layer_idx in enumerate(selected_teacher_layers):
    student_model.transformer.layer[student_layer_idx].load_state_dict(
        DistilBERT_Student_model.transformer.layer[teacher_layer_idx].state_dict()
    )

distilbert_student = BertNER(student_model,num_labels=9).to(device)#åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
model_size = sum(param.numel() * param.element_size() for param in distilbert_student.parameters())
print(f"Distilbert Model size: {model_size/(1024**2):.2f}MB")

optimizer = optim.Adam(distilbert_student.parameters(), lr=2e-4)
avg_loss_distilbert = train_with_distillation(
    model=distilbert_student,
    teacher_model=distilbert,        # ä¼  None
    dataloader=data_loader,
    optimizer=optimizer,
    device=device,
    alpha=0.3,
    T_m=2.0
)

#æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ•ˆæœ
total_loss = 0
criterion = nn.CrossEntropyLoss(ignore_index=-100)
for step, batch in enumerate(data_loader_test, 1):#len(dataloader) = 469
  input_ids = batch['input_ids'].to(device)
  attention_mask = batch['attention_mask'].to(device)
  labels = batch['labels'].to(device)
  logits = distilbert_student(input_ids, attention_mask)
  ce_loss = criterion(logits.view(-1, distilbert_student.num_labels), labels.view(-1))
  total_loss += ce_loss.item()
total_loss

